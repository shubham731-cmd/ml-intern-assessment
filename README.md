# ğŸ§  Trigram Language Model (N = 3)

This project implements a **Trigram Language Model** from scratch as part of the **AI/ML Intern Assignment**.  
The model learns trigram probabilities from text and generates new text using **probabilistic sampling** instead of deterministic word selection.

---

## ğŸ“‚ Project Structure

```
ml-assignment/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ alice_in_wonderland.txt        # Training corpus
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ nagram_model.py                 # Core model implementation
â”‚   â”œâ”€â”€ generate.py                    # Script to train & generate text
â”‚   â””â”€â”€ download_clean_alice.py        # Optional auto-downloader
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_ngram.py                  # Basic correctness tests
â”‚
â””â”€â”€ evaluation.md                      # Design choices (1-page summary)
```

---

## âœ… Setup Instructions

### 1. Install dependencies

```
pip install -r requirements.txt
```

If using the optional download script:

```
pip install requests
```

---

## ğŸ“¥ Download the Training Corpus

### Option A â€” Automatic (recommended)

```
python ml-assignment/src/download_clean_alice.py
```

### Option B â€” Manual

1. Download **"Aliceâ€™s Adventures in Wonderland"** (plain text) from Project Gutenberg  
2. Save it as:

```
ml-assignment/data/alice_in_wonderland.txt
```

---

## ğŸš€ Train Model & Generate Text

Run the generator script:

```
python ml-assignment/src/generate.py
```

This will:

âœ… load the cleaned book  
âœ… train the trigram model  
âœ… generate 3 example text samples  

Example output:

```
=== Sample #1 ===
the project gutenberg license 1 e 4 do not charge anything for copies of this agreement and any <unk> format must <unk> the rattling teacups would change to <unk> to notice this question but hurriedly went on that begins with

=== Sample #2 ===
the project gutenberg electronic work within 90 days of receipt of the trees behind him or next day maybe the footman s head with great <unk> and had been before she got to grow up any more and here alice

=== Sample #3 ===
the project gutenberg license for all that said the mouse in the sea cried the mock turtle yawned and shut his note book <unk> out a box of comfits luckily the salt water had not a bit afraid of them
```

---

## ğŸ§ª Run Tests

From project root:

```
pytest -q
```

Expected output:

```
3 passed
```

---

## ğŸ› ï¸ Model Features

âœ… Trigram count dictionary  
âœ… Probabilistic sampling using normalized frequencies  
âœ… Text preprocessing  
âœ… Unknown word handling via `<unk>` token  
âœ… Start and end padding tokens (`<s1> <s2> </s>`)  
âœ… Generates varied outputs across runs  

---

## ğŸ§  Summary of Design Choices (short version)

### âœ… Preprocessing
- convert text to lowercase  
- remove punctuation via regex  
- tokenize by whitespace  

### âœ… Vocabulary + Unknown Tokens
- word counts collected
- rare words (â‰¤ threshold) mapped to `<unk>`

### âœ… Trigram Storage Structure
```
(w1, w2) -> { w3: count }
```

### âœ… Probability Estimation
```
P(w3 | w1, w2) = count(w1,w2,w3) / total(w1,w2)
```

### âœ… Generation Strategy
- start from `<s1>, <s2>`
- repeatedly sample next word
- stop on `</s>` or max length or unseen context

### âœ… Trade-offs
âœ” simple, clean, readable  
âœ” suitable for assignment  
âœ– no smoothing  
âœ– no true sentence boundary detection  
âœ– basic tokenizer  

---

## ğŸ”§ Possible Extensions (if evaluated further)

âœ… Laplace or Kneserâ€“Ney smoothing  
âœ… Perplexity computation  
âœ… Backoff or interpolation  
âœ… Sentence segmentation  
âœ… Better tokenizer  

---

## ğŸ“Œ Purpose of This Submission

This project demonstrates:

âœ… understanding of probabilistic language models  
âœ… ability to implement core NLP logic without libraries  
âœ… clean Python coding & organization  
âœ… reasoning about design trade-offs  

---

## ğŸ Final Notes

You can now:

âœ… run the model  
âœ… generate text  
âœ… run tests  
âœ… submit confidently

